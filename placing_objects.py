# -*- coding: utf-8 -*-
"""Placing_objects.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jceyLi3QBAxYRSr2RdVY7IF0JVpL3Y6u
"""

!apt-get install -y -qq software-properties-common python-software-properties module-init-tools
!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null
!apt-get update -qq 2>&1 > /dev/null
!apt-get -y install -qq google-drive-ocamlfuse fuse
from google.colab import auth
auth.authenticate_user()
from oauth2client.client import GoogleCredentials
creds = GoogleCredentials.get_application_default()
import getpass
!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL
vcode = getpass.getpass()
!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}

!mkdir -p drive
!google-drive-ocamlfuse drive

import numpy as np
import os
from os import listdir
from os.path import isfile, join
import cv2
import scipy
import itertools
import pickle
import matplotlib.pyplot as plt
import time
import copy
import shutil
import random
import tensorflow as tf
import datetime
import scipy.misc
import imageio

orig_images = np.load('drive/orig_images.npy')
cropped = np.load('drive/cropped.npy')
image_hole = np.load('drive/images_hole.npy')

def transform(img):
    img = np.divide(np.add(img,1),2.0)
    return img
  
def process_oneimg(img):
  img = np.subtract(np.multiply(img , 2),1.0)
  return img


def next_batch(length,orig,hole,crop):
  batch = 4
  perm = np.arange(length)
  np.random.shuffle(perm)
  crop_train = [crop[perm[i]] for i in range(batch)]
  hole_train = [hole[perm[i]] for i in range(batch)]
  orig_train = [orig[perm[i]] for i in range(batch)]
  
  crop_train = process_oneimg(crop_train)
  hole_train = process_oneimg(hole_train)
  orig_train = process_oneimg(orig_train)
  g1_feed = np.concatenate([crop_train,hole_train],axis=3)
  return g1_feed,crop_train,orig_train,hole_train

def next_test_batch(length,orig,hole,crop):
  batch = 4
  perm = np.arange(length)
  np.random.shuffle(perm)
  crop_train = [crop[perm[i]] for i in range(batch)]
  orig_train = [orig[perm[i]] for i in range(batch)]
  np.random.shuffle(perm)
  hole_train = [hole[perm[i]] for i in range(batch)]
  
  crop_train = process_oneimg(crop_train)
  hole_train = process_oneimg(hole_train)
  orig_train = process_oneimg(orig_train)
  g1_feed = np.concatenate([crop_train,hole_train],axis=3)
  return g1_feed,crop_train,orig_train,hole_train

def next_scale_batch():
  image64 = np.load('drive/image64*64.npy')
  hole64 = np.load('drive/shiftedholes.npy')
  orig = np.load('drive/orig_images.npy')
  crop_train = [image64[0] for i in range(4)]
  hole_train = [hole64[i] for i in range(4)]
  orig_train = [orig[i] for i in range(4)]
  
  crop_train = process_oneimg(crop_train)
  hole_train = process_oneimg(hole_train)
  orig_train = process_oneimg(orig_train)
  g1_feed = np.concatenate([crop_train,hole_train],axis=3)
  return g1_feed,crop_train,hole_train,orig_train

g1_feed,crop_train,hole_train = next_scale_batch()

from easydict import EasyDict as ed

cfg = ed()

cfg.IMAGE_SHAPE = [64, 64, 3]
cfg.G1_INPUT_DATA_SHAPE = cfg.IMAGE_SHAPE[:2] + [6]
cfg.BATCH_SIZE = 4
cfg.BATCH_SIZE_G2D = 4
cfg.N = 6  # number of residual blocks
cfg.WEIGHT_DECAY = 0.005
cfg.LAMBDA = 50
cfg.MAXITERATION = 5000
cfg.LOGDIR = 'drive/logs'
cfg.MODE = 'train'
cfg.RESULT_DIR = 'drive/result'

WEIGHT_DECAY = cfg.WEIGHT_DECAY

def decorated_layer(layer):
	def wrapper(self, *args,  **kwargs):
		name = kwargs.setdefault('name', self.get_unique_name(layer.__name__))
		
		if len(self.inputs) == 1:
			ipt = self.inputs[0]
		else:
			ipt = self.inputs

		output = layer(self, ipt, *args, **kwargs)
		self.layers[name] = output

		return self.feed(output)

	return wrapper

class Network(object):
	def __init__(self, dataset = None, trainable = True): #data_shape = (height, width)

		self.__trainable = trainable
		if self.__trainable == True:
			assert not dataset == None
			self.__dataset = dataset

		self.__data_shape = dataset.shape
		self.__num_cls = dataset.num_cls
		self.__input_layer = tf.placeholder(tf.float32, shape = [None, self.__data_shape[0], self.__data_shape[1], self.__data_shape[2]], name = 'input')
		self.__image_info = tf.placeholder(tf.float32, shape = [None, self.__num_cls], name = "label")
		self.layers = {'input' : self.__input_layer, 'label' : self.__image_info}
		self.inputs = []
		
		self.setup()

	def feed(self,*args):
		assert len(args) != 0
		self.inputs = []

		for ipt in args:
			if isinstance(ipt, str):
				try:
					ipt = self.layers[ipt]
					print(ipt)
				except KeyError:
					print('Existing layers:',self.layers.keys())
					raise KeyError('Unknown layers %s' %ipt)
			else:
				print(ipt)
			self.inputs.append(ipt)
			
		return self

	def l2_regularizer(self, weight_decay = WEIGHT_DECAY, scope = None):
		def regularizer(tensor):
			with tf.name_scope(scope, default_name='l2_regularizer',values=[tensor]):
				l2_weight = tf.convert_to_tensor(weight_decay, dtype = tensor.dtype.base_dtype, name='weight_decay')
				return tf.multiply(l2_weight, tf.nn.l2_loss(tensor), name='value')
		return regularizer

	def weight_variable(self, shape, variable_name, scope, collection, reuse, trainable):
		if trainable:
			regularizer = self.l2_regularizer(scope = scope)
		else:
			regularizer = None

		with tf.variable_scope('weight', reuse = reuse):
			var = tf.get_variable(scope + '/' + variable_name, shape = shape, trainable = trainable, collections = collection, regularizer = regularizer)
			tf.summary.histogram(scope + '/weight', var)
			return var

	def bias_variable(self, shape, variable_name, scope, collection, reuse, trainable):
		if trainable:
			regularizer = self.l2_regularizer(scope = scope)
		else:
			regularizer = None

		with tf.variable_scope('bias', reuse = reuse):
			var = tf.get_variable(scope + '/' + variable_name, trainable = trainable, shape = shape, collections = collection, regularizer = regularizer)
			tf.summary.histogram(scope + '/bias', var)
			return var

	def __append(self, appendList, variables):
		if not appendList is None:
			assert isinstance(appendList, list)
			assert isinstance(variables, list)
			assert len(variables) > 0
			for item in variables:
				appendList.append(item)


	@decorated_layer
	def  	conv2d(self, input_data, k_w, k_d, s_w, s_h, name, collection = None, 
				scope = None, relu = True, padding = 'SAME', appendList = None, reuse = False, trainable = True):

		depth = input_data.get_shape().as_list()[-1]

		if reuse == True:
			assert not scope is None

		if scope is None:
			scope = name

		#kernel
		with tf.name_scope(scope):
			kernel = self.weight_variable([k_w, k_w, depth, k_d], 'kernel', scope, collection, reuse = reuse, trainable = trainable)
			bias = self.bias_variable([k_d], 'b', scope, collection, reuse = reuse, trainable = trainable)
			conv2d = tf.nn.conv2d(input_data, kernel, strides = [1, s_h, s_w, 1], padding = padding)

			self.__append(appendList, [kernel, bias])

			if relu:
				return tf.nn.relu(tf.nn.bias_add(conv2d, bias))
			else:
				return tf.nn.bias_add(conv2d, bias)

	@decorated_layer
	def reshape(self, input_data, *data_shape, name = None):
		assert len(data_shape) > 0
		return tf.reshape(input_data, data_shape)

	@decorated_layer
	def conv2d_tran(self, input_data, k_w, k_d, s_w, s_h, name, output_shape = None, 
			scope = None, collection = None, relu = True, padding = 'SAME', appendList = None, reuse = False, trainable = True):

		depth = input_data.get_shape().as_list()[-1]

		if reuse == True:
			assert not scope is None

		if scope is None:
			scope = name

		if output_shape is None:
			output_shape = input_data.get_shape().as_list()
			output_shape[0] = cfg.BATCH_SIZE
			output_shape[1] *= s_h
			output_shape[2] *= s_w
			output_shape[3] = k_d


		#kernel
		with tf.name_scope(scope):
			kernel = self.weight_variable([k_w, k_w, k_d, depth], 'kernel', scope, collection, reuse = reuse, trainable = trainable)
			bias = self.bias_variable([k_d], 'b', scope, collection, reuse = reuse, trainable = trainable)
			conv2d = tf.nn.conv2d_transpose(input_data, kernel, output_shape, strides = [1, s_h, s_w, 1], padding = padding)

			self.__append(appendList, [kernel, bias])

			if relu:
				return tf.nn.relu(tf.nn.bias_add(conv2d, bias))
			else:
				return tf.nn.bias_add(conv2d, bias)

	@decorated_layer
	def max_pooling(self, input_data, name, padding = 'SAME'):
		return tf.nn.max_pool(input_data, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1],padding = padding, name = name)


	@decorated_layer
	def fc(self, input_data, output_dim, name, collection = None, scope = None, relu = True, appendList = None, reuse = False, trainable = True):
		assert not isinstance(input_data, list)

		if reuse == True:
			assert not scope is None

		if scope is None:
			scope = name

		shape = input_data.get_shape().as_list()
		
		size = 1

		for i in shape[1:]:
			size *= i

		with tf.name_scope(scope):
			if len(shape) == 4:
				input_data = tf.reshape(input_data, [-1, size])
		
			w = self.weight_variable([size, output_dim], 'w', scope, collection, reuse = reuse, trainable = trainable)
			b = self.bias_variable([output_dim], 'b', scope, collection, reuse = reuse, trainable = trainable)

			self.__append(appendList, [w, b])

			if relu:
				op = tf.nn.relu_layer
			else:
				op = tf.nn.xw_plus_b

			return op(input_data, w, b)


	@decorated_layer
	def soft_max(self, input_data, name, labels = None, loss = True):
		#print(input_data.get_shape().as_list())
		if loss:
			if labels is None:
				labels = self.layers['label']
			return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = labels, logits = input_data), name = name)
		else:
			return tf.nn.softmax(input_data, name = name)

	@decorated_layer
	def sigmoid(self, input_data, name, labels = None, loss = True):
		if loss:
			if labels is None:
				labels = self.layers['label']
			return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = input_data, labels = labels), name = name)
		else:
			return tf.nn.sigmoid(input_data, name = name)
		

	@decorated_layer
	def drop_out(self, input_data, name, keep_prob_name):
		kp = tf.placeholder(tf.float32, name = keep_prob_name)
		self.layers[keep_prob_name] = kp
		return tf.nn.dropout(input_data, keep_prob = kp, name = name)

	@decorated_layer
	def producer(self, input_data, output_queue, name):
		output_queue.put(input_data)
		return input_data

	@decorated_layer
	def add(self, input_data, name):
		assert len(input_data) == 2
		return tf.add(input_data[0], input_data[1], name = name)

	@decorated_layer
	def concatenate(self, input_data, name, axis = 1):
		assert len(input_data) >= 2
		return tf.concat(input_data, axis = axis)

	@decorated_layer
	def leaky_relu(self, input_data, name, alpha = 0.2):
		return tf.nn.relu(input_data) - alpha * tf.nn.relu(-input_data)

	@decorated_layer
	def weight_sum(self, input_data, name, collection, stddev = 0.1):
		shape = input_data[0].get_shape().as_list()
		for data in input_data:
			assert data.get_shape().as_list() == shape

		weight_variables = [tf.multiply(weight_variable(shape, collection, stddev = stddev), input_data[i]) for i in len(input_data)]
		return tf.reduce_sum(weight_variables, axis = 1)

	@decorated_layer

	def batch_normalization(self, input_data, name, scope = None, relu = True, decay = 0.9, epsilon = 1e-5, updates_collections = tf.GraphKeys.UPDATE_OPS, trainable = False, appendList = None, reuse = False):
		with tf.variable_scope(scope, reuse = reuse):
			if reuse == True:
				assert not scope is None
			temp_layer =  tf.contrib.layers.batch_norm(input_data, decay = decay, scale = True, 
													center=True, variables_collections = scope, epsilon = epsilon, is_training = trainable)
		if relu:
			return tf.nn.relu(temp_layer)
		else:
			return temp_layer

	@decorated_layer
	def stop_gradient(self, input_data, name):
		return tf.stop_gradient(input_data, name = name)

	@decorated_layer
	def tanh(self, input_data, name):
		return tf.tanh(input_data)

	def setup(self):
		raise NotImplementedError('Function setup(self) must be implemented!')


	def get_unique_name(self, layer_name):
		count = len([name for name, _ in self.layers.items() if name.startswith(layer_name)])
		new_name = layer_name + '_' + str(count + 1)
		return new_name

tf.reset_default_graph()
class Pose_GAN(Network):
	def __init__(self, traing1ornot=True):
		self.inputs = []
		self.g1_input = tf.placeholder(tf.float32, shape = [cfg.BATCH_SIZE] + cfg.G1_INPUT_DATA_SHAPE, name = 'g1_input')
		self.ia_input = tf.placeholder(tf.float32, shape = [cfg.BATCH_SIZE] + cfg.IMAGE_SHAPE, name = 'ia_input')
		self.ib_input = tf.placeholder(tf.float32, shape = [cfg.BATCH_SIZE] + cfg.IMAGE_SHAPE, name = 'ib_input')
		self.traing1ornot = traing1ornot
		self.N = cfg.N
		self.im_width = cfg.G1_INPUT_DATA_SHAPE[1]
		self.im_height = cfg.G1_INPUT_DATA_SHAPE[0]
		self.g2_var = []
		self.d_var = []
		self.layers = {'g1_input': self.g1_input, 'ia_input': self.ia_input, 'ib_input': self.ib_input}
		self.__setup()

	def __setup(self):

		#=============G1 encoder============
		print('G1 encoder')
		with tf.name_scope('G1'):
			(self.feed('g1_input')
				 .conv2d(3, 128, 1, 1, name = 'conv', trainable = self.traing1ornot)
				 .conv2d(3, 128, 1, 1, name = 'block1_conv1', trainable = self.traing1ornot)
				 .conv2d(3, 128, 1, 1, name = 'block1_conv2', trainable = self.traing1ornot))
			(self.feed('conv', 'block1_conv2')
				 .add(name = 'add_1')
				 .conv2d(3, 256, 2, 2, name = 'down_sample1', relu = False, trainable = self.traing1ornot)
				 .conv2d(3, 256, 1, 1, name = 'block2_conv1', trainable = self.traing1ornot)
				 .conv2d(3, 256, 1, 1, name = 'block2_conv2', trainable = self.traing1ornot))
			(self.feed('down_sample1', 'block2_conv2')
				 .add(name = 'add_2')
				 .conv2d(3, 384, 2, 2, name = 'down_sample2', relu = False, trainable = self.traing1ornot)
				 .conv2d(3, 384, 1, 1, name = 'block3_conv1', trainable = self.traing1ornot)
				 .conv2d(3, 384, 1, 1, name = 'block3_conv2', trainable = self.traing1ornot))
			(self.feed('down_sample2', 'block3_conv2')
				 .add(name = 'add_3')
				 .conv2d(3, 512, 2, 2, name = 'down_sample3', relu = False, trainable = self.traing1ornot)
				 .conv2d(3, 512, 1, 1, name = 'block4_conv1', trainable = self.traing1ornot)
				 .conv2d(3, 512, 1, 1, name = 'block4_conv2', trainable = self.traing1ornot))
			(self.feed('down_sample3', 'block4_conv2')
				 .add(name = 'add_4')
				 .conv2d(3, 640, 2, 2, name = 'down_sample4', relu = False, trainable = self.traing1ornot)
				 .conv2d(3, 640, 1, 1, name = 'block5_conv1', trainable = self.traing1ornot)
				 .conv2d(3, 640, 1, 1, name = 'block5_conv2', trainable = self.traing1ornot))
			(self.feed('down_sample4', 'block5_conv2')
				 .add(name = 'add_5')
				 .conv2d(3, 768, 2, 2, name = 'down_sample5', relu = False, trainable = self.traing1ornot)
				 .conv2d(3, 768, 1, 1, name = 'block6_conv1', trainable = self.traing1ornot)
				 .conv2d(3, 768, 1, 1, name = 'block6_conv2', trainable = self.traing1ornot))
			(self.feed('down_sample5', 'block6_conv2')
				 .add(name = 'add_6')
				 .fc(64, name = 'fc_1', relu = False, trainable = self.traing1ornot))

			W = self.im_width
			H = self.im_height
			for _ in range(self.N - 1):
				W //= 2
				H //= 2

		#=============G1 decoder============
			print('=============G1 decoder=============')
			(self.feed('fc_1').fc(W * H * 768, name = 'fc2', relu = False, trainable = self.traing1ornot)
				 .reshape(cfg.BATCH_SIZE, W, H, 768, name = 'reshape'))
			(self.feed('reshape', 'block6_conv2')
				 .add(name = 'skip_add_1')
				 .conv2d_tran(3, 768, 1, 1, name = 'block1_dconv1', trainable = self.traing1ornot)
				 .conv2d_tran(3, 768, 1, 1, name = 'block1_dconv2', trainable = self.traing1ornot))
			(self.feed('skip_add_1', 'block1_dconv2')
				 .add(name = 'back_add_1')
				 .conv2d_tran(3, 640, 2, 2, name = 'up_sample1', relu = False, trainable = self.traing1ornot))
			(self.feed('up_sample1', 'block5_conv2')
				 .add(name = 'skip_add_2')
				 .conv2d_tran(3, 640, 1, 1, name = 'block2_dconv1', trainable = self.traing1ornot)
				 .conv2d_tran(3, 640, 1, 1, name = 'block2_dconv2', trainable = self.traing1ornot))
			(self.feed('skip_add_2', 'block2_dconv2')
				 .add(name = 'back_add_2')
				 .conv2d_tran(3, 512, 2, 2, name = 'up_sample2', relu = False, trainable = self.traing1ornot))
			(self.feed('up_sample2', 'block4_conv2')
				 .add(name = 'skip_add_3')
				 .conv2d_tran(3, 512, 1, 1, name = 'block3_dconv1', trainable = self.traing1ornot)
				 .conv2d_tran(3, 512, 1, 1, name = 'block3_dconv2', trainable = self.traing1ornot))
			(self.feed('skip_add_3', 'block3_dconv2')
				 .add(name = 'back_add_3')
				 .conv2d_tran(3, 384, 2, 2, name = 'up_sample3', relu = False, trainable = self.traing1ornot))
			(self.feed('up_sample3', 'block3_conv2')
				 .add(name = 'skip_add_4')
				 .conv2d_tran(3, 384, 1, 1, name = 'block4_dconv1', trainable = self.traing1ornot)
				 .conv2d_tran(3, 384, 1, 1, name = 'block4_dconv2', trainable = self.traing1ornot))
			(self.feed('skip_add_4', 'block4_dconv2')
				 .add(name = 'back_add_4')
				 .conv2d_tran(3, 256, 2, 2, name = 'up_sample4', relu = False, trainable = self.traing1ornot))
			(self.feed('up_sample4', 'block2_conv2')
				 .add(name = 'skip_add_5')
				 .conv2d_tran(3, 256, 1, 1, name = 'block5_dconv1', trainable = self.traing1ornot)
				 .conv2d_tran(3, 256, 1, 1, name = 'block5_dconv2', trainable = self.traing1ornot))
			(self.feed('skip_add_5', 'block5_dconv2')
				 .add(name = 'back_add_5')
				 .conv2d_tran(3, 128, 2, 2, name = 'up_sample5', relu = False, trainable = self.traing1ornot))
			(self.feed('up_sample5', 'block1_conv2')
				 .add(name = 'skip_add_6')
				 .conv2d_tran(3, 128, 1, 1, name = 'block6_dconv1', trainable = self.traing1ornot)
				 .conv2d_tran(3, 128, 1, 1, name = 'block6_dconv2', trainable = self.traing1ornot))
			(self.feed('up_sample5', 'block1_conv2')
				 .add(name = 'back_add_6')
				 .conv2d_tran(3, 3, 1, 1, name = 'dconv_out', relu = False, trainable = self.traing1ornot)
				 .tanh(name = 'g1_result'))

			#=============G2 encoder============
		with tf.name_scope('G2'):
			print('=============G2 encoder=============')
			(self.feed('g1_result').stop_gradient(name = 'barrier'))
			(self.feed('ia_input', 'barrier')
				 .concatenate(name = 'concat', axis = -1)
				 .conv2d(3, 128, 1, 1, name = 'conv_g2', appendList = self.g2_var)
				 .conv2d(3, 128, 1, 1, name = 'g2_block1_conv1', appendList = self.g2_var)
				 .conv2d(3, 128, 1, 1, name = 'g2_block1_conv2', appendList = self.g2_var))
			(self.feed('conv_g2', 'g2_block1_conv2')
				 .add(name = 'g2_add_1')
				 .conv2d(3, 256, 2, 2, name = 'g2_down_sample1', relu = False, appendList = self.g2_var)
				 .conv2d(3, 256, 1, 1, name = 'g2_block2_conv1', appendList = self.g2_var)
				 .conv2d(3, 256, 1, 1, name = 'g2_block2_conv2', appendList = self.g2_var))
			(self.feed('g2_down_sample1', 'g2_block2_conv2')
				 .add(name = 'g2_add_2')
				 .conv2d(3, 384, 2, 2, name = 'g2_down_sample2', relu = False, appendList = self.g2_var)
				 .conv2d(3, 384, 1, 1, name = 'g2_block3_conv1', appendList = self.g2_var)
				 .conv2d(3, 384, 1, 1, name = 'g2_block3_conv2', appendList = self.g2_var))
			(self.feed('g2_down_sample2', 'g2_block3_conv2')
				 .add(name = 'g2_add_3')
				 .conv2d(3, 512, 2, 2, name = 'g2_down_sample3', relu = False, appendList = self.g2_var)
				 .conv2d(3, 512, 1, 1, name = 'g2_middle_conv1', appendList = self.g2_var)
				 .conv2d(3, 512, 1, 1, name = 'g2_middle_conv2', appendList = self.g2_var))


			#=============G2 decoder============
			print('=============G2 decoder=============')
			(self.feed('g2_down_sample3', 'g2_middle_conv2')
				 .add(name = 'g2_add_4')
				 .conv2d_tran(3, 384, 2, 2, name = 'g2_up_sample1', relu = False, appendList = self.g2_var))
			(self.feed('g2_up_sample1', 'g2_block3_conv2')
				 .add(name = 'g2_skip_add1')
				 .conv2d_tran(3, 384, 1, 1, name = 'g2_block1_dconv1', appendList = self.g2_var)
				 .conv2d_tran(3, 384, 1, 1, name = 'g2_block1_dconv2', appendList = self.g2_var))
			(self.feed('g2_skip_add1', 'g2_block1_dconv2')
				 .add(name = 'g2_back_add1')
				 .conv2d_tran(3, 256, 2, 2, name = 'g2_up_sample2', relu = False, appendList = self.g2_var))
			(self.feed('g2_up_sample2', 'g2_block2_conv2')
				 .add(name = 'g2_skip_add2')
				 .conv2d_tran(3, 256, 1, 1, name = 'g2_block2_dconv1', appendList = self.g2_var)
				 .conv2d_tran(3, 256, 1, 1, name = 'g2_block2_dconv2', appendList = self.g2_var))
			(self.feed('g2_skip_add2', 'g2_block2_dconv2')
				 .add(name = 'g2_back_add2')
				 .conv2d_tran(3, 128, 2, 2, name = 'g2_up_sample3', relu = False, appendList = self.g2_var))
			(self.feed('g2_up_sample3', 'g2_block1_conv2')
				 .add(name = 'g2_skip_add3')
				 .conv2d_tran(3, 128, 1, 1, name = 'g2_block3_dconv1', appendList = self.g2_var)
				 .conv2d_tran(3, 128, 1, 1, name = 'g2_block3_dconv2', appendList = self.g2_var))
			(self.feed('g2_skip_add3', 'g2_block3_dconv2')
				 .add(name = 'g2_back_add3')
				 .conv2d_tran(3, 3, 1, 1, name = 'g2_dconv_out', relu = False, appendList = self.g2_var)
				 .tanh(name = 'g2_result'))

		#=============Final output============
		print('=============Final output=============')
		(self.feed('barrier', 'g2_result')
			 .add(name = 'final_output'))

		#=============Discriminator============
		print('=============Discriminator=============')
		with tf.variable_scope('Discriminator'):
			(self.feed('ia_input', 'ib_input')
				 .concatenate(name = 'd_real_input', axis = -1)
				 .conv2d(5, 64, 2, 2, name = 'd_real_conv1', scope = 'd_conv_1', relu = False)
				 .leaky_relu(name = 'd_real_lrelu1')
				 .conv2d(5, 128, 2, 2, name = 'd_real_conv2', scope = 'd_conv_2', relu = False)
				 .batch_normalization(name = 'd_real_bn1', scope = 'd_bn1',relu = False, trainable = True, updates_collections = None)
				 .leaky_relu(name = 'd_real_lrelu2')
				 .conv2d(5, 256, 2, 2, name = 'd_real_conv3', scope = 'd_conv_3', relu = False)
				 .batch_normalization(name = 'd_real_bn2', scope = 'd_bn2', relu = False, trainable = True, updates_collections = None)
				 .leaky_relu(name = 'd_real_lrelu3')
				 .conv2d(5, 512, 2, 2, name = 'd_real_conv4', scope = 'd_conv_4', relu = False)
				 .batch_normalization(name = 'd_real_bn3', scope = 'd_bn3', relu = False, trainable = True, updates_collections = None)
				 .leaky_relu(name = 'd_real_lrelu4')
				 .fc(1, name = 'logit_real', scope = 'logit', relu = False)
				 .sigmoid(name = 'd_real', loss = False))

			(self.feed('ia_input', 'final_output')
				 .concatenate(name = 'd_fake_input', axis = -1)
				 .conv2d(5, 64, 2, 2, name = 'd_fake_conv1', scope = 'd_conv_1', relu = False, reuse = True)
				 .leaky_relu(name = 'd_fake_lrelu1')
				 .conv2d(5, 128, 2, 2, name = 'd_fake_conv2', scope = 'd_conv_2', relu = False, reuse = True)
				 .batch_normalization(name = 'd_fake_bn1', scope = 'd_bn1',relu = False, trainable = True, updates_collections = None, reuse = True)
				 .leaky_relu(name = 'd_fake_lrelu2')
				 .conv2d(5, 256, 2, 2, name = 'd_fake_conv3', scope = 'd_conv_3', relu = False, reuse = True)
				 .batch_normalization(name = 'd_fake_bn2', scope = 'd_bn2', relu = False, trainable = True, updates_collections = None, reuse = True)
				 .leaky_relu(name = 'd_fake_lrelu3')
				 .conv2d(5, 512, 2, 2, name = 'd_fake_conv4', scope = 'd_conv_4', relu = False, reuse = True)
				 .batch_normalization(name = 'd_fake_bn3', scope = 'd_bn3', relu = False, trainable = True, updates_collections = None, reuse = True)
				 .leaky_relu(name = 'd_fake_lrelu4')
				 .fc(1, name = 'logit_fake', scope = 'logit', relu = False, reuse = True)
				 .sigmoid(name = 'd_fake', loss = False))


			t_var = tf.trainable_variables()
			self.d_var = [var for var in t_var if 'd_' in var.name]

	@property
	def d_fake(self):
		return self.layers['d_fake']

	@property
	def d_real(self):
		return self.layers['d_real']

	@property
	def g1_output(self):
		return self.layers['g1_result']

	@property
	def g2_output(self):
		return self.layers['g2_result']

	@property
	def final_output(self):
		return self.layers['final_output']

	
	def build_loss(self):
		#=============g1 loss============
		l1_distance = tf.abs(self.layers['g1_result'] - self.layers['ib_input'])

		self.layers['g1_loss'] = tf.reduce_mean(tf.reduce_sum(l1_distance, axis = [1, 2, 3]))

		#=============discriminator loss============
		(self.feed('logit_real')
			 .sigmoid(name = 'real_loss', labels = tf.ones_like(self.layers['logit_real']), loss = True))
		(self.feed('logit_fake')
			 .sigmoid(name = 'fake_loss', labels = tf.zeros_like(self.layers['logit_fake']), loss = True))
		self.layers['d_loss'] = tf.reduce_mean(self.layers['fake_loss'] + self.layers['real_loss'])

		#=============g2 loss============
		# (self.feed('logit_fake')
		# 	 .sigmoid(name = 'g2_adv_loss', labels = tf.ones_like(self.layers['logit_fake']), loss = True))
		likely_hood = -tf.reduce_mean(tf.log(self.layers['d_fake']))

		l1_distance2 = tf.reduce_sum(tf.abs(self.layers['final_output'] - self.layers['ib_input']), axis = [1, 2, 3])
		#self.layers['g2_loss'] = tf.reduce_mean(self.layers['g2_adv_loss']) + cfg.LAMBDA * tf.reduce_mean(l1_distance2)
		self.layers['g2_loss'] = likely_hood + cfg.LAMBDA * tf.reduce_mean(l1_distance2)
		#=============l2 regularization loss============
		# self.layers['l2_reg_loss'] = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)

		return self.layers['g1_loss'], self.layers['g2_loss'], self.layers['d_loss']

if __name__ == '__main__':
	model = Pose_GAN()
	a, b, c = model.build_loss()
	print('==========================')
	counter = 0
	for var in model.g2_var:
		counter += 1
		print(var.name)
	print('number of variables:', counter)
	print('==========================')
	for var in model.d_var:
		print(var.name)

tf.reset_default_graph()
perm = np.arange(len(orig_images))
np.random.shuffle(perm)

train_orig = [orig_images[i] for i in perm[0:int(0.8*len(orig_images))]] 
train_hole = [image_hole[i] for i in perm[0:int(0.8*len(orig_images))]] 
train_crop = [cropped[i] for i in perm[0:int(0.8*len(orig_images))]] 

valid_orig = [orig_images[i] for i in perm[int(0.8*len(orig_images)):]] 
valid_hole = [image_hole[i] for i in perm[int(0.8*len(orig_images)):]] 
valid_crop = [cropped[i] for i in perm[int(0.8*len(orig_images)):]]
  
model = Pose_GAN()
g1_loss, g2_loss, d_loss= model.build_loss()
tf.summary.scalar("g1loss", g1_loss)
tf.summary.scalar("g2loss", g2_loss)
tf.summary.scalar("dloss", d_loss)

sess = tf.Session()

train_g1 = tf.train.AdamOptimizer(learning_rate=2e-5, beta1=0.5).minimize(g1_loss)
train_g2 = tf.train.AdamOptimizer(learning_rate=1e-6, beta1=0.5).minimize(g2_loss, var_list = model.g2_var)
train_d = tf.train.AdamOptimizer(learning_rate=1e-6, beta1=0.5).minimize(d_loss, var_list = model.d_var)

if not os.path.exists(cfg.LOGDIR):
    os.makedirs(cfg.LOGDIR)
    
saver = tf.train.Saver(max_to_keep=2)
summary_writer = tf.summary.FileWriter(cfg.LOGDIR, sess.graph)
sess.run(tf.global_variables_initializer())
ckpt = tf.train.get_checkpoint_state(cfg.LOGDIR)

start_itr = 0

if ckpt and ckpt.model_checkpoint_path:
    saver.restore(sess, ckpt.model_checkpoint_path)
    print("Model restored...")
    start_itr = int(ckpt.model_checkpoint_path.split('-')[1])
    print("starting from iteration", start_itr)

print("Setting up summary op...")
summary_merge = tf.summary.merge_all()
if not os.path.exists(cfg.RESULT_DIR):
    os.makedirs(cfg.RESULT_DIR)

g1_feed,crop_train,hole_train,orig_train = next_scale_batch()
feed_dict = {model.g1_input: g1_feed, model.ia_input:crop_train,
                     model.ib_input: orig_train}
final_output, g2_out, g1_out = sess.run([model.final_output, model.g2_output, model.g1_output],
                                                    feed_dict=feed_dict)



'''
if (start_itr < cfg.MAXITERATION):
    # step 1: train g1
    for itr in range(start_itr, cfg.MAXITERATION):
        g1_feed,crop_train,orig_train,hole_train = next_batch(len(train_orig),train_orig,train_hole,train_crop)

        feed_dict = {model.g1_input: g1_feed, model.ia_input:crop_train,
                     model.ib_input: orig_train}
        sess.run(train_g1, feed_dict=feed_dict)
        if itr %10 == 0:
            train_loss, summaryString = sess.run([g1_loss,summary_merge],feed_dict=feed_dict)
            summary_writer.add_summary(summaryString,itr)
            train_loss = sess.run([g1_loss],feed_dict=feed_dict) # remove later
            print("training loss is", train_loss, "itr",itr)

        if itr == cfg.MAXITERATION - 1 or itr%10000==0:
            if itr==cfg.MAXITERATION-1:
                print("Training of G1 done. At iteration ", itr)
            #saver.save(sess, cfg.LOGDIR + "/model.ckpt", global_step=itr)

        if itr % 1000 == 0:
            final_output, g2_out, g1_out = sess.run([model.final_output, model.g2_output, model.g1_output],
                                                    feed_dict=feed_dict)

            size = final_output.shape[0]
            dir_name = cfg.RESULT_DIR + '/g1_iter_' + str(itr) + 'at' + str(datetime.datetime.now())
            if not os.path.exists(dir_name):
                os.makedirs(dir_name)
            for i in range(10):
                name = dir_name + '/sample' + str(i + 1) + 'finalout.jpg'
                imageio.imwrite(name, transform(final_output[i]))
                name = dir_name + '/sample' + str(i + 1) + 'g2out.jpg'
                imageio.imwrite(name, transform(g2_out[i]))
                name = dir_name + '/sample' + str(i + 1) + 'g1out.jpg'
                imageio.imwrite(name, transform(g1_out[i]))
                name_cond = dir_name + '/sample' + str(i + 1) + 'conditionalimg.jpg'
                imageio.imwrite(name_cond, transform(crop_train[i, :, :, :]))
                name_target = dir_name + '/sample' + str(i + 1) + 'target.jpg'
                imageio.imwrite(name_target, transform(orig_train[i, :, :, :]))
                name_target = dir_name + '/sample' + str(i + 1) + 'hole.jpg'
                imageio.imwrite(name_target, transform(hole_train[i, :, :, :]))
            
            
            g1_feed,crop_train,orig_train,hole_train = next_test_batch(len(valid_orig),valid_orig,valid_hole,valid_crop)
            feed_dict = {model.g1_input: g1_feed, model.ia_input:crop_train,
                     model.ib_input: orig_train}
            val_g1loss = sess.run(g1_loss,feed_dict=feed_dict)
            final_output, g2_out, g1_out = sess.run([model.final_output, model.g2_output, model.g1_output],
                                                    feed_dict=feed_dict)
            size = final_output.shape[0]
            dir_name = cfg.RESULT_DIR + '/g1_iter_test' + str(itr) + 'at' + str(datetime.datetime.now())
            if not os.path.exists(dir_name):
                os.makedirs(dir_name)
            for i in range(10):
                name = dir_name + '/sample' + str(i + 1) + 'finalout.jpg'
                imageio.imwrite(name, transform(final_output[i]))
                name = dir_name + '/sample' + str(i + 1) + 'g2out.jpg'
                imageio.imwrite(name, transform(g2_out[i]))
                name = dir_name + '/sample' + str(i + 1) + 'g1out.jpg'
                imageio.imwrite(name, transform(g1_out[i]))
                name_cond = dir_name + '/sample' + str(i + 1) + 'conditionalimg.jpg'
                imageio.imwrite(name_cond, transform(crop_train[i, :, :, :]))
                name_target = dir_name + '/sample' + str(i + 1) + 'target.jpg'
                imageio.imwrite(name_target, transform(orig_train[i, :, :, :]))
                name_target = dir_name + '/sample' + str(i + 1) + 'hole.jpg'
                imageio.imwrite(name_target, transform(hole_train[i, :, :, :]))
            print("Validation G1 loss at itr ", itr, " is ", val_g1loss)




saver = tf.train.Saver(max_to_keep=2)
# step 2: train g2 and d
for itr in range(cfg.MAXITERATION-1, 2*cfg.MAXITERATION):
    g1_feed,crop_train,orig_train,hole_train = next_batch(len(train_orig),train_orig,train_hole,train_crop)
    feed_dict = {model.g1_input: g1_feed, model.ia_input:crop_train,
                     model.ib_input: orig_train}             
    sess.run([train_g2], feed_dict=feed_dict)

    sess.run([train_d], feed_dict=feed_dict)

    if itr %10 == 0:
        fake_score, real_score, g2loss, dloss, summaryString = sess.run([model.d_fake, model.d_real, g2_loss, d_loss, summary_merge],feed_dict=feed_dict)
        #fake_score, real_score, g2loss, dloss = sess.run([model.d_fake, model.d_real, g2_loss, d_loss],feed_dict=feed_dict)
        avg_fake_score = np.mean(fake_score)
        avg_real_score = np.mean(real_score)
        summary_writer.add_summary(summaryString,itr)


        print("g2 loss:", g2loss, "|d loss", dloss, "|d real:", avg_real_score, "|d fake", avg_fake_score, "|iteration ", itr)

    if itr == 2*cfg.MAXITERATION - 1 :
        saver.save(sess, cfg.LOGDIR + "/model.ckpt", global_step=itr)

    if itr % 1000 == 0:
        final_output, g2_out, g1_out = sess.run([model.final_output, model.g2_output, model.g1_output], feed_dict=feed_dict)
        size = final_output.shape[0]

        dir_name = cfg.RESULT_DIR + '/g2_iter_' + str(itr) + 'at' + str(datetime.datetime.now())
        if not os.path.exists(dir_name):
            os.makedirs(dir_name)
        for i in range(size):
            name = dir_name + '/sample' + str(i + 1) + 'finalout.jpg'
            imageio.imwrite(name, transform(final_output[i]))
            name = dir_name + '/sample' + str(i + 1) + 'g2out.jpg'
            imageio.imwrite(name, transform(g2_out[i]))
            name = dir_name + '/sample' + str(i + 1) + 'g1out.jpg'
            imageio.imwrite(name, transform(g1_out[i]))
            name_cond = dir_name + '/sample' + str(i + 1) + 'conditionalimg.jpg'
            imageio.imwrite(name_cond, transform(crop_train[i, :, :, :]))
            name_target = dir_name + '/sample' + str(i + 1) + 'target.jpg'
            imageio.imwrite(name_target, transform(orig_train[i, :, :, :]))
            name_target = dir_name + '/sample' + str(i + 1) + 'hole.jpg'
            imageio.imwrite(name_target, transform(hole_train[i, :, :, :]))
        
        g1_feed,crop_train,orig_train,hole_train = next_test_batch(len(valid_orig),valid_orig,valid_hole,valid_crop)
        feed_dict = {model.g1_input: g1_feed, model.ia_input:crop_train,
                     model.ib_input: orig_train}
        g2lossvalue, dlossvalue = sess.run([g2_loss, d_loss], feed_dict=feed_dict)
        final_output, g2_out, g1_out = sess.run([model.final_output, model.g2_output, model.g1_output],
                                                    feed_dict=feed_dict)
        size = final_output.shape[0]
        dir_name = cfg.RESULT_DIR + '/g2_iter_test' + str(itr) + 'at' + str(datetime.datetime.now())
        if not os.path.exists(dir_name):
                os.makedirs(dir_name)
        for i in range(size):
                name = dir_name + '/sample' + str(i + 1) + 'finalout.jpg'
                imageio.imwrite(name, transform(final_output[i]))
                name = dir_name + '/sample' + str(i + 1) + 'g2out.jpg'
                imageio.imwrite(name, transform(g2_out[i]))
                name = dir_name + '/sample' + str(i + 1) + 'g1out.jpg'
                imageio.imwrite(name, transform(g1_out[i]))
                name_cond = dir_name + '/sample' + str(i + 1) + 'conditionalimg.jpg'
                imageio.imwrite(name_cond, transform(crop_train[i, :, :, :]))
                name_target = dir_name + '/sample' + str(i + 1) + 'target.jpg'
                imageio.imwrite(name_target, transform(orig_train[i, :, :, :]))
                name_target = dir_name + '/sample' + str(i + 1) + 'hole.jpg'
                imageio.imwrite(name_target, transform(hole_train[i, :, :, :]))  
                
        print("Validation G2 D loss at itr ", itr, " is ", g2lossvalue, " and ", dlossvalue)
        
'''

plt.imshow(transform(final_output[3]))

#plt.imsave('drive/40_1.png',transform(final_output[0])/np.max(final_output[0]))
#plt.imsave('drive/40_2.png',transform(final_output[1])/np.max(final_output[1]))
plt.imsave('drive/40_3.png',final_output[2])
plt.imsave('drive/40_4.png',transform(final_output[3])/np.max(final_output[3]))

plt.imsave('drive/hole_4.png',transform(hole_train[3]))


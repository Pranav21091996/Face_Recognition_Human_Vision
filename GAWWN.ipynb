{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAWWN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "BGdVS4W6iCK5",
        "colab_type": "code",
        "outputId": "1b4eee01-6c0a-453d-df90-66068cbf7112",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 110842 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.1-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GPf_ju_uiGYO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ogAc6t5_GffU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1103
        },
        "outputId": "fba34d5a-8054-486e-9305-b86b378f26da"
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorlayer"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorlayer\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/55/2dc51f4a8e772240e63c442de06762ddefd0631399f446b6895be5e2590d/tensorlayer-1.11.1-py2.py3-none-any.whl (316kB)\n",
            "\u001b[K    100% |████████████████████████████████| 317kB 23.6MB/s \n",
            "\u001b[?25hCollecting scikit-image<0.15,>=0.14 (from tensorlayer)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/90/553120309c53bdfca25c9c50769ae40a538a90c24db8c082468aec898d00/scikit_image-0.14.1-cp36-cp36m-manylinux1_x86_64.whl (25.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 25.3MB 1.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: progressbar2<3.39,>=3.38 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (3.38.0)\n",
            "Requirement already satisfied: tqdm<4.29,>=4.23 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (4.28.1)\n",
            "Requirement already satisfied: imageio<2.5,>=2.3 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (2.4.1)\n",
            "Requirement already satisfied: wrapt<1.11,>=1.10 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (1.10.11)\n",
            "Collecting requests<2.21,>=2.19 (from tensorlayer)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/17/5cbb026005115301a8fb2f9b0e3e8d32313142fe8b617070e7baad20554f/requests-2.20.1-py2.py3-none-any.whl (57kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 19.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn<0.21,>=0.19 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (0.20.1)\n",
            "Requirement already satisfied: numpy<1.16,>=1.14 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (1.14.6)\n",
            "Collecting matplotlib<3.1,>=2.2 (from tensorlayer)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/07/16d781df15be30df4acfd536c479268f1208b2dfbc91e9ca5d92c9caf673/matplotlib-3.0.2-cp36-cp36m-manylinux1_x86_64.whl (12.9MB)\n",
            "\u001b[K    100% |████████████████████████████████| 12.9MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml<4.3,>=4.2 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (4.2.5)\n",
            "Requirement already satisfied: scipy<1.2,>=1.1 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (1.1.0)\n",
            "Requirement already satisfied: dask[array]>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer) (0.20.2)\n",
            "Requirement already satisfied: networkx>=1.8 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer) (2.2)\n",
            "Collecting pillow>=4.3.0 (from scikit-image<0.15,>=0.14->tensorlayer)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer) (0.6.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer) (1.11.0)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer) (1.0.1)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2<3.39,>=3.38->tensorlayer) (2.3.0)\n",
            "Requirement already satisfied: idna<2.8,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<2.21,>=2.19->tensorlayer) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<2.21,>=2.19->tensorlayer) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<2.21,>=2.19->tensorlayer) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<2.21,>=2.19->tensorlayer) (2018.11.29)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib<3.1,>=2.2->tensorlayer) (0.10.0)\n",
            "Collecting kiwisolver>=1.0.1 (from matplotlib<3.1,>=2.2->tensorlayer)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/a7/88719d132b18300b4369fbffa741841cfd36d1e637e1990f27929945b538/kiwisolver-1.0.1-cp36-cp36m-manylinux1_x86_64.whl (949kB)\n",
            "\u001b[K    100% |████████████████████████████████| 952kB 18.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<3.1,>=2.2->tensorlayer) (2.5.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<3.1,>=2.2->tensorlayer) (2.3.0)\n",
            "Requirement already satisfied: toolz>=0.7.3; extra == \"array\" in /usr/local/lib/python3.6/dist-packages (from dask[array]>=0.9.0->scikit-image<0.15,>=0.14->tensorlayer) (0.9.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=1.8->scikit-image<0.15,>=0.14->tensorlayer) (4.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib<3.1,>=2.2->tensorlayer) (40.6.3)\n",
            "\u001b[31myellowbrick 0.9 has requirement matplotlib<3.0,>=1.5.1, but you'll have matplotlib 3.0.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mspacy 2.0.18 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mgoogle-colab 0.0.1a1 has requirement requests~=2.18.0, but you'll have requests 2.20.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mcufflinks 0.14.6 has requirement plotly>=3.0.0, but you'll have plotly 1.12.12 which is incompatible.\u001b[0m\n",
            "Installing collected packages: kiwisolver, matplotlib, pillow, scikit-image, requests, tensorlayer\n",
            "  Found existing installation: matplotlib 2.1.2\n",
            "    Uninstalling matplotlib-2.1.2:\n",
            "      Successfully uninstalled matplotlib-2.1.2\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "  Found existing installation: scikit-image 0.13.1\n",
            "    Uninstalling scikit-image-0.13.1:\n",
            "      Successfully uninstalled scikit-image-0.13.1\n",
            "  Found existing installation: requests 2.18.4\n",
            "    Uninstalling requests-2.18.4:\n",
            "      Successfully uninstalled requests-2.18.4\n",
            "Successfully installed kiwisolver-1.0.1 matplotlib-3.0.2 pillow-5.3.0 requests-2.20.1 scikit-image-0.14.1 tensorlayer-1.11.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Zjt52drTia67",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import datetime\n",
        "import dateutil.tz\n",
        "import argparse\n",
        "from shutil import copyfile\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.layers import utils\n",
        "import tensorflow.contrib.eager as tfe\n",
        "from tensorflow.python.client import timeline\n",
        "import tensorlayer as tl\n",
        "import cv2\n",
        "import scipy.misc\n",
        "import PIL\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E3txZnnJelLy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image = np.load('drive/image.npy')\n",
        "label = np.load('drive/labels.npy')\n",
        "box = np.load('drive/bbox.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8brQ0Q53f_kr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Data = image\n",
        "label = label\n",
        "bbox = box\n",
        "epochs_completed = 0\n",
        "index_in_epoch = 0\n",
        "num_examples = len(image)\n",
        "\n",
        "def next_batch(batch_size):\n",
        "\n",
        "  global Data\n",
        "  global label\n",
        "  global bbox\n",
        "  global index_in_epoch\n",
        "  global epochs_completed\n",
        "\n",
        "  start = index_in_epoch\n",
        "  index_in_epoch += batch_size\n",
        "\n",
        "\t# when all trainig data have been already used, it is reorder randomly\n",
        "  if index_in_epoch > num_examples:\n",
        "\t\t# finished epoch\n",
        "    epochs_completed += 1\n",
        "\t\t# shuffle the data\n",
        "    perm = np.arange(num_examples)\n",
        "    np.random.shuffle(perm)\n",
        "    Data = [Data[i] for i in perm]\n",
        "    label = [label[i] for i in perm]\n",
        "    bbox = [bbox[i] for i in perm]\n",
        "\t\t# start next epoch\n",
        "    start = 0\n",
        "    index_in_epoch = batch_size\n",
        "    assert batch_size <= num_examples\n",
        "  end = index_in_epoch\n",
        "  return Data[start:end], label[start:end], bbox[start:end]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ANMhW95CihDX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16        # training batch size\n",
        "batch_size = 16\n",
        "MAX_ITER = 15000           # maximum number of iterations\n",
        "IMG_WIDTH, IMG_HEIGHT = 64, 64      # image dimensions\n",
        "IMG_CHANNELS = 3                    # image channels\n",
        "LR_D = 3e-4                    # learning rate discriminator\n",
        "LR_G = 1e-4                  # learning rate generator\n",
        "BETA1_D = 0.5             # beta1 value for Adam optimizer (discriminator)\n",
        "BETA1_G = 0.5              # beta1 value for Adam optimizer (generator)\n",
        "n_batches = int(num_examples/batch_size)\n",
        "Z_DIM = 128                # dimensionality of the z vector (input to G, incompressible noise)\n",
        "LABEL_DIM = 200                      # dimensionality of the label vector (axis 1)\n",
        "weight_init = tf.truncated_normal_initializer(stddev=0.02)\n",
        "LOGDIR = 'drive/logs'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ceigYEc6piDV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample_z(m, n):\n",
        "    return np.random.normal(loc=0.0, scale=1.0, size=(m, n)).astype(np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z6UB-YCAmn-o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conv_cond_concat(x, y):\n",
        "    \"\"\"Concatenate conditioning vector on feature map axis.\"\"\"\n",
        "    x_shapes = tf.shape(x)\n",
        "    y_shapes = y.get_shape()\n",
        "    y = tf.reshape(y, (x_shapes[0], 1, 1, y_shapes[1]))\n",
        "    z = tf.ones([x_shapes[0], x_shapes[1], x_shapes[2], y_shapes[1]])\n",
        "    y = tf.cast(y,'float32')\n",
        "    y = y*z\n",
        "\n",
        "    return tf.concat([x , y], 3)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hVfqYnIDnJfB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def spatial_transformer_network(input_fmap, theta, out_dims=None, **kwargs):\n",
        "    \"\"\"\n",
        "    Spatial Transformer Network layer implementation as described in [1].\n",
        "    The layer is composed of 3 elements:\n",
        "    - localisation_net: takes the original image as input and outputs \n",
        "      the parameters of the affine transformation that should be applied\n",
        "      to the input image.\n",
        "    - affine_grid_generator: generates a grid of (x,y) coordinates that \n",
        "      correspond to a set of points where the input should be sampled \n",
        "      to produce the transformed output.\n",
        "    - bilinear_sampler: takes as input the original image and the grid\n",
        "      and produces the output image using bilinear interpolation.\n",
        "    Input\n",
        "    -----\n",
        "    - input_fmap: output of the previous layer. Can be input if spatial\n",
        "      transformer layer is at the beginning of architecture. Should be \n",
        "      a tensor of shape (B, H, W, C). \n",
        "    - theta: affine transform tensor of shape (B, 6). Permits cropping, \n",
        "      translation and isotropic scaling. Initialize to identity matrix. \n",
        "      It is the output of the localization network.\n",
        "    Returns\n",
        "    -------\n",
        "    - out_fmap: transformed input feature map. Tensor of size (B, H, W, C).\n",
        "    Notes\n",
        "    -----\n",
        "    [1]: 'Spatial Transformer Networks', Jaderberg et. al,\n",
        "         (https://arxiv.org/abs/1506.02025)\n",
        "    \"\"\"\n",
        "    # grab input dimensions\n",
        "    B = tf.shape(input_fmap)[0]\n",
        "    H = tf.shape(input_fmap)[1]\n",
        "    W = tf.shape(input_fmap)[2]\n",
        "    C = tf.shape(input_fmap)[3]\n",
        "\n",
        "    # reshape theta to (B, 2, 3)\n",
        "    theta = tf.reshape(theta, [B, 2, 3])\n",
        "\n",
        "    # generate grids of same size or upsample/downsample if specified\n",
        "    if out_dims:\n",
        "        out_H = out_dims[0]\n",
        "        out_W = out_dims[1]\n",
        "        batch_grids = affine_grid_generator(out_H, out_W, theta)\n",
        "    else:\n",
        "        batch_grids = affine_grid_generator(H, W, theta)\n",
        "\n",
        "    x_s = batch_grids[:, 0, :, :]\n",
        "    y_s = batch_grids[:, 1, :, :]\n",
        "\n",
        "    # sample input with grid to get output\n",
        "    out_fmap = bilinear_sampler(input_fmap, x_s, y_s)\n",
        "\n",
        "    return out_fmap\n",
        "\n",
        "def tf_compute_transformation_matrix(bbox, shape=16., img_height=64):\n",
        "    rel_factor = float(shape)/img_height\n",
        "    x, y, w, h = rel_factor*bbox[0], rel_factor*bbox[1], rel_factor*bbox[2], rel_factor*bbox[3]\n",
        "\n",
        "    t_x = (x+0.5*w-0.5*shape)/(0.5*shape)\n",
        "    t_y = (y+0.5*h-0.5*shape)/(0.5*shape)\n",
        "\n",
        "    scale_x = (w / shape)\n",
        "    scale_y = (h / shape)\n",
        "\n",
        "    line0 = tf.stack((scale_x, 0.0, t_x))\n",
        "    line1 = tf.stack((0.0, scale_y, t_y))\n",
        "    transformation_matrix = tf.concat((line0, line1), axis=0)\n",
        "    transformation_matrix = tf.reshape(transformation_matrix, (2, 3))\n",
        "\n",
        "    return transformation_matrix \n",
        "  \n",
        "  \n",
        "def tf_compute_transformation_matrix_inverse(bbox, shape=16., img_height=64):\n",
        "    rel_factor = float(shape)/img_height\n",
        "    x, y, w, h = rel_factor*bbox[0], rel_factor*bbox[1], rel_factor*bbox[2], rel_factor*bbox[3]\n",
        "\n",
        "    scale_x = (shape / w)\n",
        "    scale_y = (shape / h)\n",
        "\n",
        "    t_x = (shape - 2 * x) / w - 1\n",
        "    t_y = (shape - 2 * y) / h - 1\n",
        "\n",
        "    line0 = tf.stack((scale_x, 0.0, t_x))\n",
        "    line1 = tf.stack((0.0, scale_y, t_y))\n",
        "\n",
        "    transformation_matrix = tf.concat((line0, line1), axis=0)\n",
        "    transformation_matrix = tf.reshape(transformation_matrix, (2, 3))\n",
        "\n",
        "    return transformation_matrix  \n",
        "  \n",
        "def sample_gen_label(mb_size):\n",
        "    labels = np.random.multinomial(1, 10*[0.1], size=mb_size)\n",
        "    return labels.astype(np.float32)\n",
        "\n",
        "\n",
        "def sample_gen_label_sorted(mb_size, label_dim=10):\n",
        "    labels = np.zeros((mb_size, label_dim))\n",
        "    for idx in range(label_dim):\n",
        "        labels[idx*label_dim:idx*label_dim+label_dim, idx] = 1\n",
        "    return labels.astype(np.float32)\n",
        "\n",
        "\n",
        "def sample_bbox(mb_size):\n",
        "    pos_box_x = np.random.randint(low=0, high=44, size=(mb_size, 1))\n",
        "    pos_box_y = np.random.randint(low=0, high=44, size=(mb_size, 1))\n",
        "\n",
        "    coin = np.random.binomial(1, 0.5)\n",
        "    if coin < 0.1:\n",
        "        scale_box_x = np.random.randint(low=8, high=16, size=(mb_size, 1))\n",
        "    elif 0.1 < coin < 0.8:\n",
        "        scale_box_x = np.random.randint(low=12, high=18, size=(mb_size, 1))\n",
        "    else:\n",
        "        scale_box_x = np.random.randint(low=16, high=21, size=(mb_size, 1))\n",
        "    scale_box_y = np.random.randint(low=18, high=21, size=(mb_size, 1))\n",
        "\n",
        "    boxes = np.concatenate((pos_box_x, pos_box_y, scale_box_x, scale_box_y), axis=1)\n",
        "    return boxes.astype(np.float32)\n",
        "\n",
        "\n",
        "def sample_bbox_sorted(mb_size, sort_size=False):\n",
        "    pos_box_x = np.zeros((mb_size, 1))\n",
        "    for idx in range(100):\n",
        "        pos_box_x[idx, 0] = (idx % 10 + 1) * 4\n",
        "\n",
        "    pos_box_y = np.zeros((mb_size, 1))\n",
        "    for idx in range(10):\n",
        "        pos_box_y[idx * 10:idx * 10 + 10, 0] = (idx + 1) * 4\n",
        "\n",
        "    if sort_size:\n",
        "        scale_box_x = np.zeros((mb_size, 1))\n",
        "        for idx in range(100):\n",
        "            scale_box_x[idx, 0] = (idx % 10 + 1) + 10\n",
        "\n",
        "        scale_box_y = np.zeros((mb_size, 1))\n",
        "        for idx in range(10):\n",
        "            scale_box_y[idx * 10:idx * 10 + 10, 0] = (idx + 1) + 10\n",
        "    else:\n",
        "        scale_box_x = np.random.randint(low=16, high=21, size=(mb_size, 1))\n",
        "        scale_box_y = np.random.randint(low=18, high=21, size=(mb_size, 1))\n",
        "\n",
        "    boxes = np.concatenate((pos_box_x, pos_box_y, scale_box_x, scale_box_y), axis=1)\n",
        "    return boxes.astype(np.float32)\n",
        "\n",
        "\n",
        "def sample_generator_input(mb_size, n, sort_labels=False, sort_location=False, sort_bbox_size=False):\n",
        "    _z = sample_z(mb_size, n)\n",
        "    _Y = sample_gen_label_sorted(mb_size) if sort_labels else sample_gen_label(mb_size)\n",
        "\n",
        "    if sort_bbox_size:\n",
        "        _bbox = sample_bbox_sorted(mb_size, sort_size=True)\n",
        "    else:\n",
        "        _bbox = sample_bbox_sorted(mb_size) if sort_location else sample_bbox(mb_size)\n",
        "\n",
        "\n",
        "    return _z, _Y, _bbox\n",
        "\n",
        "def affine_grid_generator(height, width, theta):\n",
        "    \"\"\"\n",
        "    This function returns a sampling grid, which when\n",
        "    used with the bilinear sampler on the input feature \n",
        "    map, will create an output feature map that is an \n",
        "    affine transformation [1] of the input feature map.\n",
        "    Input\n",
        "    -----\n",
        "    - height: desired height of grid/output. Used\n",
        "      to downsample or upsample. \n",
        "    - width: desired width of grid/output. Used\n",
        "      to downsample or upsample. \n",
        "    - theta: affine transform matrices of shape (num_batch, 2, 3). \n",
        "      For each image in the batch, we have 6 theta parameters of \n",
        "      the form (2x3) that define the affine transformation T.\n",
        "    Returns\n",
        "    -------\n",
        "    - normalized gird (-1, 1) of shape (num_batch, 2, H, W).\n",
        "      The 2nd dimension has 2 components: (x, y) which are the \n",
        "      sampling points of the original image for each point in the\n",
        "      target image.\n",
        "    Note\n",
        "    ----\n",
        "    [1]: the affine transformation allows cropping, translation, \n",
        "         and isotropic scaling.\n",
        "    \"\"\"\n",
        "    # grab batch size\n",
        "    num_batch = tf.shape(theta)[0]\n",
        "\n",
        "    # create normalized 2D grid\n",
        "    x = tf.linspace(-1.0, 1.0, width)\n",
        "    y = tf.linspace(-1.0, 1.0, height)\n",
        "    x_t, y_t = tf.meshgrid(x, y)\n",
        "\n",
        "    # flatten\n",
        "    x_t_flat = tf.reshape(x_t, [-1])\n",
        "    y_t_flat = tf.reshape(y_t, [-1])\n",
        "\n",
        "    # reshape to [x_t, y_t , 1] - (homogeneous form)\n",
        "    ones = tf.ones_like(x_t_flat)\n",
        "    sampling_grid = tf.stack([x_t_flat, y_t_flat, ones])\n",
        "\n",
        "    # repeat grid num_batch times\n",
        "    sampling_grid = tf.expand_dims(sampling_grid, axis=0)\n",
        "    sampling_grid = tf.tile(sampling_grid, tf.stack([num_batch, 1, 1]))\n",
        "\n",
        "    # cast to float32 (required for matmul)\n",
        "    theta = tf.cast(theta, 'float32')\n",
        "    sampling_grid = tf.cast(sampling_grid, 'float32')\n",
        "\n",
        "    # transform the sampling grid - batch multiply\n",
        "    batch_grids = tf.matmul(theta, sampling_grid)\n",
        "    # batch grid has shape (num_batch, 2, H*W)\n",
        "\n",
        "    # reshape to (num_batch, H, W, 2)\n",
        "    batch_grids = tf.reshape(batch_grids, [num_batch, 2, height, width])\n",
        "\n",
        "    return batch_grids\n",
        "\n",
        "def bilinear_sampler(img, x, y):\n",
        "    \"\"\"\n",
        "    Performs bilinear sampling of the input images according to the \n",
        "    normalized coordinates provided by the sampling grid. Note that \n",
        "    the sampling is done identically for each channel of the input.\n",
        "    To test if the function works properly, output image should be\n",
        "    identical to input image when theta is initialized to identity\n",
        "    transform.\n",
        "    Input\n",
        "    -----\n",
        "    - img: batch of images in (B, H, W, C) layout.\n",
        "    - grid: x, y which is the output of affine_grid_generator.\n",
        "    Returns\n",
        "    -------\n",
        "    - interpolated images according to grids. Same size as grid.\n",
        "    \"\"\"\n",
        "    # prepare useful params\n",
        "    B = tf.shape(img)[0]\n",
        "    H = tf.shape(img)[1]\n",
        "    W = tf.shape(img)[2]\n",
        "    C = tf.shape(img)[3]\n",
        "\n",
        "    max_y = tf.cast(H - 1, 'int32')\n",
        "    max_x = tf.cast(W - 1, 'int32')\n",
        "    zero = tf.zeros([], dtype='int32')\n",
        "\n",
        "    # cast indices as float32 (for rescaling)\n",
        "    x = tf.cast(x, 'float32')\n",
        "    y = tf.cast(y, 'float32')\n",
        "\n",
        "    # rescale x and y to [0, W/H]\n",
        "    x = 0.5 * ((x + 1.0) * tf.cast(W, 'float32'))\n",
        "    y = 0.5 * ((y + 1.0) * tf.cast(H, 'float32'))\n",
        "\n",
        "    # grab 4 nearest corner points for each (x_i, y_i)\n",
        "    # i.e. we need a rectangle around the point of interest\n",
        "    x0 = tf.cast(tf.floor(x), 'int32')\n",
        "    x1 = x0 + 1\n",
        "    y0 = tf.cast(tf.floor(y), 'int32')\n",
        "    y1 = y0 + 1\n",
        "\n",
        "    # clip to range [0, H/W] to not violate img boundaries\n",
        "    x0 = tf.clip_by_value(x0, zero, max_x)\n",
        "    x1 = tf.clip_by_value(x1, zero, max_x)\n",
        "    y0 = tf.clip_by_value(y0, zero, max_y)\n",
        "    y1 = tf.clip_by_value(y1, zero, max_y)\n",
        "\n",
        "    # get pixel value at corner coords\n",
        "    Ia = get_pixel_value(img, x0, y0)\n",
        "    Ib = get_pixel_value(img, x0, y1)\n",
        "    Ic = get_pixel_value(img, x1, y0)\n",
        "    Id = get_pixel_value(img, x1, y1)\n",
        "\n",
        "    # recast as float for delta calculation\n",
        "    x0 = tf.cast(x0, 'float32')\n",
        "    x1 = tf.cast(x1, 'float32')\n",
        "    y0 = tf.cast(y0, 'float32')\n",
        "    y1 = tf.cast(y1, 'float32')\n",
        "\n",
        "    # calculate deltas\n",
        "    wa = (x1 - x) * (y1 - y)\n",
        "    wb = (x1 - x) * (y - y0)\n",
        "    wc = (x - x0) * (y1 - y)\n",
        "    wd = (x - x0) * (y - y0)\n",
        "\n",
        "    # add dimension for addition\n",
        "    wa = tf.expand_dims(wa, axis=3)\n",
        "    wb = tf.expand_dims(wb, axis=3)\n",
        "    wc = tf.expand_dims(wc, axis=3)\n",
        "    wd = tf.expand_dims(wd, axis=3)\n",
        "\n",
        "    # compute output\n",
        "    out = tf.add_n([wa * Ia, wb * Ib, wc * Ic, wd * Id])\n",
        "\n",
        "\n",
        "    return out  \n",
        "  \n",
        "def get_pixel_value(img, x, y):\n",
        "    \"\"\"\n",
        "    Utility function to get pixel value for coordinate\n",
        "    vectors x and y from a  4D tensor image.\n",
        "    Input\n",
        "    -----\n",
        "    - img: tensor of shape (B, H, W, C)\n",
        "    - x: flattened tensor of shape (B*H*W, )\n",
        "    - y: flattened tensor of shape (B*H*W, )\n",
        "    Returns\n",
        "    -------\n",
        "    - output: tensor of shape (B, H, W, C)\n",
        "    \"\"\"\n",
        "    shape = tf.shape(x)\n",
        "    batch_size = shape[0]\n",
        "    height = shape[1]\n",
        "    width = shape[2]\n",
        "\n",
        "    batch_idx = tf.range(0, batch_size)\n",
        "    batch_idx = tf.reshape(batch_idx, (batch_size, 1, 1))\n",
        "    b = tf.tile(batch_idx, (1, height, width))\n",
        "\n",
        "    indices = tf.stack([b, y, x], 3)\n",
        "\n",
        "    return tf.gather_nd(img, indices)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zsD86UZpEJM3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def transform(img):\n",
        "    img = np.divide(np.add(img,1),2.0)\n",
        "    return img\n",
        "  \n",
        "def process_oneimg(img):\n",
        "  img = np.subtract(np.multiply(img , 2),1.0)\n",
        "  return img\n",
        "\n",
        "def lrelu(x,alpha=0.2):\n",
        "\treturn tf.maximum(x,alpha*x)\t\n",
        "\n",
        "def conv2d(x, features, kernel=[4,4], strides=[1,2,2,1], name=\"conv_layer\"):\n",
        "\twith tf.variable_scope(name) as scope:\n",
        "\t\tweights = weight(shape=kernel + features, name=\"weights\")\n",
        "\t\tbiases = bias(shape=[features[-1]], name=\"bias\")\n",
        "\t\toutput = tf.nn.conv2d(x, weights, strides=strides, padding='SAME') \n",
        "\t\toutput = tf.nn.bias_add(output, biases)\n",
        "\t\treturn output\t\n",
        "\n",
        "def deconv2d(x, features, output_shape, kernel=[4,4], strides=[1,2,2,1], name=\"deconv_layer\"):\n",
        "\twith tf.variable_scope(name) as scope:\n",
        "\t\tweights = weight(shape=kernel + features, name=\"weights\")\n",
        "\t\tbiases = bias(shape=[features[0]], name=\"bias\")\n",
        "\t\toutput = tf.nn.conv2d_transpose(x, weights, output_shape=output_shape, strides=strides, padding='SAME') \n",
        "\t\treturn tf.reshape(tf.nn.bias_add(output, biases), output.get_shape())\n",
        "\n",
        "def bias(shape, name):\n",
        "\treturn tf.get_variable(name, shape,initializer=tf.constant_initializer(0.00000))\n",
        "\n",
        "def weight(shape, name):\n",
        "\treturn tf.get_variable(name, shape,initializer=tf.glorot_uniform_initializer())\t\n",
        "\n",
        "def dense(x, shape, name):\n",
        "\twith tf.variable_scope(name):\n",
        "\t\tweights = weight(shape, name=\"weights\")\n",
        "\t\tbiases = bias([shape[-1]], name=\"bias\")\n",
        "\t\treturn tf.matmul(x,weights) + biases\n",
        "\n",
        "def batch_norm(inputs, decay=0.9, epsilon=0.00001, scale=True, isTrain=True, name=\"batch_norm\"):\n",
        "\treturn tf.contrib.layers.batch_norm(inputs, decay=decay, scale=scale, epsilon=epsilon, updates_collections=None, is_training=isTrain, scope=name)\t\t\t\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nMY1RXbRmRAu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def discriminate(image_input, label, bounding_box,reuse):\n",
        "  with tf.variable_scope(\"discriminator\") as scope:\n",
        "    if reuse:\n",
        "      scope.reuse_variables()\n",
        "    #input = conv_cond_concat(image_input, label)\n",
        "    d_x_conv_0 = conv2d(image_input, features=[3, 32],kernel=[5,5], name=\"d_conv_layer_1\")\n",
        "    d_x_conv_0 = lrelu(d_x_conv_0)\n",
        "      \n",
        "    d_x_conv_1 = conv2d(d_x_conv_0, features=[32, 64], kernel=[5,5],name=\"d_conv_layer_2\")\n",
        "    d_x_conv_1 = batch_norm(d_x_conv_1, isTrain=True, name=\"d_batch_norm_2\")\n",
        "    d_x_conv_1 = lrelu(d_x_conv_1)\n",
        "      \n",
        "    ####################################################\n",
        "    # global pathway\n",
        "      \n",
        "    d_x_conv_global_0 = conv2d(d_x_conv_1, features=[64, 64], kernel=[5,5],name=\"d_conv_global_1\")\n",
        "    d_x_conv_global_0 = batch_norm(d_x_conv_global_0, isTrain=True, name=\"d_batch_global_1\")\n",
        "    d_x_conv_global_0 = lrelu(d_x_conv_global_0)\n",
        "      \n",
        "    d_x_conv_global_1 = conv2d(d_x_conv_global_0, features=[64, 128], kernel=[5,5],name=\"d_conv_global_2\")\n",
        "    d_x_conv_global_1 = batch_norm(d_x_conv_global_1, isTrain=True, name=\"d_batch_global_2\")\n",
        "    d_x_conv_global_1 = lrelu(d_x_conv_global_1)\n",
        "      \n",
        "    shp = [int(s) for s in d_x_conv_global_1.shape[1:]]\n",
        "    d_x_conv_global_1 = tf.reshape(d_x_conv_global_1, [-1, shp[0] * shp[1] * shp[2]])\n",
        "\n",
        "    ####################################################\n",
        "    # local pathway\n",
        "    # reshape bounding box to (16, 16) resolution\n",
        "    transf_matri = tf.map_fn(tf_compute_transformation_matrix, bounding_box)\n",
        "    local_input = spatial_transformer_network(d_x_conv_1, transf_matri, (16, 16))\n",
        "      \n",
        "    d_x_conv_local_0 = conv2d(local_input, features=[64, 64], kernel=[5,5],name=\"d_conv_local_0\")\n",
        "    d_x_conv_local_0 = batch_norm(d_x_conv_local_0, isTrain=True, name=\"d_batch_local_0\")\n",
        "    d_x_conv_local_0 = lrelu(d_x_conv_local_0)\n",
        "      \n",
        "    d_x_conv_local_1 = conv2d(d_x_conv_local_0, features=[64, 128], kernel=[5,5],name=\"d_conv_local_1\")\n",
        "    d_x_conv_local_1 = batch_norm(d_x_conv_local_1, isTrain=True, name=\"d_batch_local_1\")\n",
        "    d_x_conv_local_1 = lrelu(d_x_conv_local_1)\n",
        "      \n",
        "    shp = [int(s) for s in d_x_conv_local_1.shape[1:]]\n",
        "    d_x_conv_local_1 = tf.reshape(d_x_conv_local_1, [-1, shp[0] * shp[1] * shp[2]])\n",
        "\n",
        "    ####################################################\n",
        "    # final discriminator\n",
        "    final_input = tf.concat((d_x_conv_global_1, d_x_conv_local_1), axis=1)\n",
        "      \n",
        "    d_final_dense = dense(final_input, shape=[4096, 512], name=\"d_final_dense_1\")\n",
        "    d_final_dense = batch_norm(d_final_dense, isTrain=True, name=\"d_batch_dense_1\")\n",
        "    d_final_dense = lrelu(d_final_dense)\n",
        "      \n",
        "    d_final_pred = dense(d_final_dense, shape=[512, 1],name=\"d_final_dense_2\")\n",
        "\n",
        "    return d_final_pred,tf.nn.sigmoid(d_final_pred)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mk5bOoCTjm8c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sampler(noise_input, label, bounding_box):\n",
        "  with tf.variable_scope(\"g_net\") as scope:\n",
        "    scope.reuse_variables()\n",
        "    \n",
        "    #input = tf.concat((noise_input, label), axis=1)\n",
        "    g_dense_0 = dense(noise_input, shape=[128, 2048], name=\"g_dense_1\")\n",
        "    g_dense_0 = batch_norm(g_dense_0, isTrain=False, name=\"g_batch_norm_0\")\n",
        "    g_dense_0 = tf.nn.relu(g_dense_0)\n",
        "\n",
        "    g_dense_0 = tf.reshape(g_dense_0, [-1, 4, 4, 128])\n",
        "\n",
        "    ####################################################\n",
        "    # global pathway\n",
        "        \n",
        "    g_conv_global_0 = deconv2d(g_dense_0, features=[256, 128], output_shape=[batch_size,8,8,256], name=\"g_deconv_global_1\")\n",
        "    g_conv_global_0 = batch_norm(g_conv_global_0, isTrain=False, name=\"g_batch_global_1\")\n",
        "    g_conv_global_0 = tf.nn.relu(g_conv_global_0)\n",
        "\n",
        "    g_conv_global_1 = deconv2d(g_conv_global_0, features=[256, 256], output_shape=[batch_size,16,16,256], name=\"g_deconv_global_2\")\n",
        "    g_conv_global_1 = batch_norm(g_conv_global_1, isTrain=False, name=\"g_batch_global_2\")\n",
        "    g_conv_global_1 = tf.nn.relu(g_conv_global_1)\n",
        "        \n",
        "        \n",
        "    ####################################################\n",
        "    # local pathway\n",
        "        \n",
        "    g_conv_local_0 = deconv2d(g_dense_0, features=[256, 128], output_shape=[batch_size,8,8,256], name=\"g_deconv_local_1\")\n",
        "    g_conv_local_0 = batch_norm(g_conv_local_0, isTrain=False, name=\"g_batch_local_1\")\n",
        "    g_conv_local_0 = tf.nn.relu(g_conv_local_0)\n",
        "\n",
        "    g_conv_local_1 = deconv2d(g_conv_local_0, features=[256, 256], output_shape=[batch_size,16,16,256], name=\"g_deconv_local_2\")\n",
        "    g_conv_local_1 = batch_norm(g_conv_local_1, isTrain=False, name=\"g_batch_local_2\")\n",
        "    g_conv_local_1 = tf.nn.relu(g_conv_local_1)\n",
        "        \n",
        "    # reshape to bounding box\n",
        "    transf_matri = tf.map_fn(tf_compute_transformation_matrix_inverse, bounding_box)\n",
        "    g_conv_local_1 = spatial_transformer_network(g_conv_local_1, transf_matri, (16, 16))\n",
        "\n",
        "    ####################################################\n",
        "    # final pathway\n",
        "    final_input = tf.concat((g_conv_global_1, g_conv_local_1), axis=3)\n",
        "    g_conv_final = deconv2d(final_input, features=[256, 512], output_shape=[batch_size,32,32,256], name=\"g_deconv_final_1\")\n",
        "    g_conv_final = batch_norm(g_conv_final, isTrain=False, name=\"g_batch_final_1\")\n",
        "    g_conv_final = tf.nn.relu(g_conv_final)\n",
        "        \n",
        "    g_conv_final_2 = deconv2d(g_conv_final, features=[256, 256], output_shape=[batch_size,64,64,256], name=\"g_deconv_final_2\")\n",
        "    g_conv_final_2 = batch_norm(g_conv_final_2, isTrain=False, name=\"g_batch_final_2\")\n",
        "    g_conv_final_2 = tf.nn.relu(g_conv_final_2)\n",
        "        \n",
        "        \n",
        "    g_conv_out = deconv2d(g_conv_final_2, features=[3, 256], output_shape=[batch_size,64,64,3],strides=[1,1,1,1], name=\"g_deconv_output_1\")\n",
        "    g_conv_out= tf.nn.tanh(g_conv_out)\n",
        "\n",
        "    return g_conv_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "swgL44dWm6AN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate(noise_input, label, bounding_box):\n",
        "  with tf.variable_scope(\"g_net\") as scope:\n",
        "    #input = tf.concat((noise_input, label), axis=1)\n",
        "    g_dense_0 = dense(noise_input, shape=[128, 2048], name=\"g_dense_1\")\n",
        "    g_dense_0 = batch_norm(g_dense_0, isTrain=False, name=\"g_batch_norm_0\")\n",
        "    g_dense_0 = tf.nn.relu(g_dense_0)\n",
        "\n",
        "    g_dense_0 = tf.reshape(g_dense_0, [-1, 4, 4, 128])\n",
        "\n",
        "    ####################################################\n",
        "    # global pathway\n",
        "        \n",
        "    g_conv_global_0 = deconv2d(g_dense_0, features=[256, 128], output_shape=[batch_size,8,8,256], name=\"g_deconv_global_1\")\n",
        "    g_conv_global_0 = batch_norm(g_conv_global_0, isTrain=True, name=\"g_batch_global_1\")\n",
        "    g_conv_global_0 = tf.nn.relu(g_conv_global_0)\n",
        "\n",
        "    g_conv_global_1 = deconv2d(g_conv_global_0, features=[256, 256], output_shape=[batch_size,16,16,256], name=\"g_deconv_global_2\")\n",
        "    g_conv_global_1 = batch_norm(g_conv_global_1, isTrain=True, name=\"g_batch_global_2\")\n",
        "    g_conv_global_1 = tf.nn.relu(g_conv_global_1)\n",
        "        \n",
        "        \n",
        "    ####################################################\n",
        "    # local pathway\n",
        "        \n",
        "    g_conv_local_0 = deconv2d(g_dense_0, features=[256, 128], output_shape=[batch_size,8,8,256], name=\"g_deconv_local_1\")\n",
        "    g_conv_local_0 = batch_norm(g_conv_local_0, isTrain=True, name=\"g_batch_local_1\")\n",
        "    g_conv_local_0 = tf.nn.relu(g_conv_local_0)\n",
        "\n",
        "    g_conv_local_1 = deconv2d(g_conv_local_0, features=[256, 256], output_shape=[batch_size,16,16,256], name=\"g_deconv_local_2\")\n",
        "    g_conv_local_1 = batch_norm(g_conv_local_1, isTrain=True, name=\"g_batch_local_2\")\n",
        "    g_conv_local_1 = tf.nn.relu(g_conv_local_1)\n",
        "        \n",
        "    # reshape to bounding box\n",
        "    transf_matri = tf.map_fn(tf_compute_transformation_matrix_inverse, bounding_box)\n",
        "    g_conv_local_1 = spatial_transformer_network(g_conv_local_1, transf_matri, (16, 16))\n",
        "\n",
        "    ####################################################\n",
        "    # final pathway\n",
        "    final_input = tf.concat((g_conv_global_1, g_conv_local_1), axis=3)\n",
        "    g_conv_final = deconv2d(final_input, features=[256, 512], output_shape=[batch_size,32,32,256], name=\"g_deconv_final_1\")\n",
        "    g_conv_final = batch_norm(g_conv_final, isTrain=True, name=\"g_batch_final_1\")\n",
        "    g_conv_final = tf.nn.relu(g_conv_final)\n",
        "        \n",
        "    g_conv_final_2 = deconv2d(g_conv_final, features=[256, 256], output_shape=[batch_size,64,64,256], name=\"g_deconv_final_2\")\n",
        "    g_conv_final_2 = batch_norm(g_conv_final_2, isTrain=True, name=\"g_batch_final_2\")\n",
        "    g_conv_final_2 = tf.nn.relu(g_conv_final_2)\n",
        "        \n",
        "        \n",
        "    g_conv_out = deconv2d(g_conv_final_2, features=[3, 256], output_shape=[batch_size,64,64,3],strides=[1,1,1,1], name=\"g_deconv_output_1\")\n",
        "    g_conv_out= tf.nn.tanh(g_conv_out)\n",
        "\n",
        "    return g_conv_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0GLc5_CIoONj",
        "colab_type": "code",
        "outputId": "ebd23904-2039-4030-8ac3-5d40b80959de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "Y = tf.placeholder(tf.float32, shape=[None,LABEL_DIM], name='label')\n",
        "X = tf.placeholder(tf.float32, shape=[None,64,64,3], name='image')\n",
        "b = tf.placeholder(tf.float32, shape=[None,4], name='box')\n",
        "\n",
        "z = tf.placeholder(tf.float32, shape=[None,Z_DIM], name='z')\n",
        "Y_ = tf.placeholder(tf.float32, shape=[None,10], name='label_gen')\n",
        "b_ = tf.placeholder(tf.float32, shape=[None,4], name='box_gen')\n",
        "\n",
        "fake_images = generate(noise_input=z, label=Y, bounding_box=b)\n",
        "fake_disc_logits, fake_disc = discriminate(image_input=fake_images, label=Y, bounding_box=b, reuse=False)\n",
        "real_img_real_label_disc_logits, real_disc_real = discriminate(image_input=X, label=Y, bounding_box=b, reuse=True)\n",
        "real_img_fake_label_disc_logits, real_disc_fake = discriminate(image_input=X, label=Y, bounding_box=b_, reuse=True)\n",
        "sample = sampler(noise_input=z, label=Y, bounding_box=b)\n",
        "\n",
        "\n",
        "d_loss1 = tl.cost.sigmoid_cross_entropy(real_img_real_label_disc_logits, tf.ones_like(real_img_real_label_disc_logits), name='d1')\n",
        "d_loss2 = tl.cost.sigmoid_cross_entropy(real_img_fake_label_disc_logits,  tf.zeros_like(real_img_fake_label_disc_logits), name='d2')\n",
        "d_loss3 = tl.cost.sigmoid_cross_entropy(fake_disc_logits, tf.zeros_like(fake_disc_logits), name='d3')\n",
        "d_loss = d_loss1 + (d_loss2 + d_loss3)\n",
        "g_loss = tl.cost.sigmoid_cross_entropy(fake_disc_logits, tf.ones_like(fake_disc_logits), name='g')\n",
        "\n",
        "tf.summary.scalar(\"g_loss\", g_loss)\n",
        "tf.summary.scalar(\"d_loss\", d_loss)\n",
        "\n",
        "lr = 0.0002\n",
        "lr_decay = 0.5      # decay factor for adam, https://github.com/reedscot/icml2016/blob/master/main_cls_int.lua  https://github.com/reedscot/icml2016/blob/master/scripts/train_flowers.sh\n",
        "decay_every = 100   # https://github.com/reedscot/icml2016/blob/master/main_cls.lua\n",
        "beta1 = 0.5\n",
        "t_vars = tf.trainable_variables()\n",
        "\n",
        "d_vars = [var for var in t_vars if 'd_' in var.name]\n",
        "g_vars = [var for var in t_vars if 'g_' in var.name]\n",
        "\n",
        "d_optim = tf.train.AdamOptimizer(lr, beta1=0.5).minimize(d_loss, var_list=d_vars)\n",
        "g_optim = tf.train.AdamOptimizer(lr, beta1=0.5).minimize(g_loss, var_list=g_vars)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  saver = tf.train.Saver(max_to_keep=2)\n",
        "  summary_writer = tf.summary.FileWriter(LOGDIR, sess.graph)\n",
        "  ckpt = tf.train.get_checkpoint_state(LOGDIR)\n",
        "  epoch=0\n",
        "  if ckpt and ckpt.model_checkpoint_path:\n",
        "      saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "      print(\"Model restored...\")\n",
        "      epoch = int(ckpt.model_checkpoint_path.split('-')[1])\n",
        "      print(\"starting from iteration\", epoch)\n",
        "  z_mb, Y_gen, box_generated = sample_generator_input(BATCH_SIZE, Z_DIM)\n",
        "  #z_mb = z_new\n",
        "  #_, Y_gen, box_generated = sample_generator_input(BATCH_SIZE, Z_DIM)\n",
        "  data_1, label_1, bbox_1 = next_batch(BATCH_SIZE)\n",
        "  data_1 = process_oneimg(data_1)\n",
        "  feed_dict = {Y:label_1,X:data_1,b:bbox_1,z:z_mb,Y_:Y_gen,b_:box_generated}\n",
        "  fake_image = sess.run(sample,feed_dict=feed_dict)    \n",
        "  \n",
        "  '''\n",
        "  while epoch!=1000:\n",
        "    dl, gl = [],[]\n",
        "      \n",
        "    if epoch % 50 == 0 :\n",
        "        saver.save(sess, LOGDIR + \"/model.ckpt\", global_step=epoch)\n",
        "  \n",
        "    for i in range(n_batches):\n",
        "      z_mb, Y_gen, box_generated = sample_generator_input(BATCH_SIZE, Z_DIM)\n",
        "      data_1, label_1, bbox_1 = next_batch(BATCH_SIZE)\n",
        "      data_1 = process_oneimg(data_1)\n",
        "      feed_dict = {Y:label_1,X:data_1,b:bbox_1,z:z_mb,Y_:Y_gen,b_:box_generated}\n",
        "      _, DLOSS = sess.run([d_optim, d_loss],feed_dict=feed_dict)\n",
        "\t\t\t\n",
        "\t\t\t# Update G network\n",
        "      _, GLOSS = sess.run([g_optim, g_loss],feed_dict=feed_dict)\n",
        "\n",
        "\t\t\t# Update G network\n",
        "      _, GLOSS = sess.run([g_optim, g_loss],feed_dict=feed_dict)\n",
        "      _, GLOSS = sess.run([g_optim, g_loss],feed_dict=feed_dict)\n",
        "      \n",
        "      \n",
        "      dl.append(DLOSS)\n",
        "      gl.append(GLOSS)\n",
        "    print('discriminator_loss / generator_loss => %.2f / %.2f for step %d'%(np.mean(dl), np.mean(gl), epoch))\n",
        "    z_mb, Y_gen, box_generated = sample_generator_input(BATCH_SIZE, Z_DIM)\n",
        "    data_1, label_1, bbox_1 = next_batch(BATCH_SIZE)\n",
        "    data_1 = process_oneimg(data_1)\n",
        "    feed_dict = {Y:label_1,X:data_1,b:bbox_1,z:z_mb,Y_:Y_gen,b_:box_generated}\n",
        "    fake_image = sess.run(sample,feed_dict=feed_dict)\n",
        "    plt.imsave('drive/new_result/'+str(epoch)+'.jpg',transform(fake_image[0]))\n",
        "    epoch+=1\n",
        "\n",
        "  '''"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from drive/logs/model.ckpt-150\n",
            "Model restored...\n",
            "starting from iteration 150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2U-efEsuo1sA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "73440024-7391-4291-ba98-6f4465e0a506"
      },
      "cell_type": "code",
      "source": [
        "i = 12\n",
        "x = bbox_1[i][0]\n",
        "y = bbox_1[i][1]\n",
        "width = bbox_1[i][2]\n",
        "height = bbox_1[i][3]\n",
        "im = transform(fake_image[i])\n",
        "im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
        "im = cv2.rectangle(im,(x,y),(x+width,y+height),(0,255,0),1).astype('uint8')\n",
        "\n",
        "print(np.max(im))\n",
        "#cv2.imwrite('drive/'+str(101)+'.png',im)\n",
        "#im1 = im/255\n",
        "im = im + im1\n",
        "#im = im.astype('uint8')\n",
        "print(np.max(im))\n",
        "plt.imshow(im)\n",
        "plt.imsave('drive/'+str(100)+'.jpg',im)"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAegAAAFOCAYAAABNFY7/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njBpYUMuoAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f1e559d49b0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "8E8m7RAdT2Ec",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "list = z_mb[8]\n",
        "z_new = [list for i in range(16)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k4-qYrYcW-z4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.imsave('drive/'+str(100)+'.jpg',im)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yjYLCXMcXLH7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "156e28e6-73bc-4fb2-fbed-084ade825620"
      },
      "cell_type": "code",
      "source": [
        "np.min(im)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "metadata": {
        "id": "sIzO79_Jasg5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}